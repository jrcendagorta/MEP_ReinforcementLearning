{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrcendagorta/.local/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['colors', 'e', 'size', 'random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "import random\n",
    "from matplotlib  import cm\n",
    "from matplotlib import colors\n",
    "import random\n",
    "import math\n",
    "colormap = plt.cm.jet\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the energy function for the PES\n",
    "# This will also introduce an assciated cost\n",
    "\n",
    "def energy_point_2(x,y):\n",
    "    C = 1\n",
    "    l = 1\n",
    "    X0 = 1\n",
    "    \n",
    "    aa = [-1.0,-1.0,-6.5,0.7]\n",
    "    bb = [0.0,0.0,11.0,0.6]\n",
    "    cc = [-10.0,-10.0,-6.5,0.7]\n",
    "    AA = [-200.0,-100.0,-170.0,15.0]\n",
    "    XX = [1.0,0.0,-0.5,-1.0]\n",
    "    YY = [0.0,0.5,1.5,1.0]\n",
    "    #energy=np.zeros((np.size(x),np.size(y)))\n",
    "    energy = l*np.square(np.square(x)-X0)-C*np.multiply(x,y)+0.5*np.square(y)+C**2/2*np.square(x)\n",
    "    \n",
    "    \n",
    "    #energy=np.add(energy,et)\n",
    "    return energy\n",
    "\n",
    "def energy_mesh(x,y):\n",
    "    aa = [-1.0,-1.0,-6.5,0.7]\n",
    "    bb = [0.0,0.0,11.0,0.6]\n",
    "    cc = [-10.0,-10.0,-6.5,0.7]\n",
    "    AA = [-200.0,-100.0,-170.0,15.0]\n",
    "    XX = [1.0,0.0,-0.5,-1.0]\n",
    "    YY = [0.0,0.5,1.5,1.0]\n",
    "    #energy=np.zeros((np.size(x),np.size(y)))\n",
    "    energy = AA[0]*np.exp(aa[0]*np.square(x-XX[0])+bb[0]*np.multiply(x-XX[0],y-YY[0])+cc[0]*np.square(y-YY[0]))\n",
    "    energy += AA[1]*np.exp(aa[1]*np.square(x-XX[1])+bb[1]*np.multiply(x-XX[1],y-YY[1])+cc[1]*np.square(y-YY[1]))\n",
    "    energy += AA[2]*np.exp(aa[2]*np.square(x-XX[2])+bb[2]*np.multiply(x-XX[2],y-YY[2])+cc[2]*np.square(y-YY[2]))\n",
    "    energy += AA[3]*np.exp(aa[3]*np.square(x-XX[3])+bb[3]*np.multiply(x-XX[3],y-YY[3])+cc[3]*np.square(y-YY[3]))\n",
    "    \n",
    "    force_x = AA[0]*np.exp(aa[0]*np.square(x-XX[0])+bb[0]*np.multiply(x-XX[0],y-YY[0])+cc[0]*np.square(y-YY[0]))*(aa[0]*(x-XX[0])*2.0 + bb[0]*(y-YY[0]))\n",
    "    force_x += AA[1]*np.exp(aa[1]*np.square(x-XX[1])+bb[1]*np.multiply(x-XX[1],y-YY[1])+cc[1]*np.square(y-YY[1]))*(aa[1]*(x-XX[1])*2.0 + bb[1]*(y-YY[1]))\n",
    "    force_x += AA[2]*np.exp(aa[2]*np.square(x-XX[2])+bb[2]*np.multiply(x-XX[2],y-YY[2])+cc[2]*np.square(y-YY[2]))*(aa[2]*(x-XX[2])*2.0 + bb[2]*(y-YY[2]))\n",
    "    force_x += AA[3]*np.exp(aa[3]*np.square(x-XX[3])+bb[3]*np.multiply(x-XX[3],y-YY[3])+cc[3]*np.square(y-YY[3]))*(aa[3]*(x-XX[3])*2.0 + bb[3]*(y-YY[2]))\n",
    "    \n",
    "    force_y = AA[0]*np.exp(aa[0]*np.square(x-XX[0])+bb[0]*np.multiply(x-XX[0],y-YY[0])+cc[0]*np.square(y-YY[0]))*(cc[0]*(y-YY[0])*2.0 + bb[0]*(x-XX[0]))\n",
    "    force_y += AA[1]*np.exp(aa[1]*np.square(x-XX[1])+bb[1]*np.multiply(x-XX[1],y-YY[1])+cc[1]*np.square(y-YY[1]))*(cc[1]*(y-YY[1])*2.0 + bb[1]*(x-XX[1]))\n",
    "    force_y += AA[2]*np.exp(aa[2]*np.square(x-XX[2])+bb[2]*np.multiply(x-XX[2],y-YY[2])+cc[2]*np.square(y-YY[2]))*(cc[2]*(y-YY[2])*2.0 + bb[2]*(x-XX[2]))\n",
    "    force_y += AA[3]*np.exp(aa[3]*np.square(x-XX[3])+bb[3]*np.multiply(x-XX[3],y-YY[3])+cc[3]*np.square(y-YY[3]))*(cc[3]*(y-YY[3])*2.0 + bb[3]*(x-XX[2]))\n",
    "    #energy=np.add(energy,et)\n",
    "    return energy, force_x, force_y\n",
    "\n",
    "def energy_point(x,y):\n",
    "    aa = np.array([-1.0,-1.0,-6.5,0.7])\n",
    "    bb = np.array([0.0,0.0,11.0,0.6])\n",
    "    cc = np.array([-10.0,-10.0,-6.5,0.7])\n",
    "    AA = np.array([-200.0,-100.0,-170.0,15.0])\n",
    "    XX = np.array([1.0,0.0,-0.5,-1.0])\n",
    "    YY = np.array([0.0,0.5,1.5,1.0])\n",
    "    dX = x-XX\n",
    "    dY = y-YY\n",
    "    dX2 = np.multiply(np.square(dX),aa)\n",
    "    dY2 = np.multiply(np.square(dY),cc)\n",
    "    dXY = np.multiply(np.multiply(dX,dY),bb)\n",
    "    eXP = np.exp(np.add(np.add(dX2,dXY),dY2))\n",
    "    e = np.multiply(AA,eXP)\n",
    "    dfX = np.add(2.0*np.multiply(aa,dX),np.multiply(bb,dY))\n",
    "    dfY = np.add(2.0*np.multiply(cc,dY),np.multiply(bb,dX))\n",
    "    fx = np.multiply(e,dfX)\n",
    "    fy = np.multiply(e,dfY)\n",
    "    \n",
    "    dxx = np.add(np.multiply(fx,dfX),2.0*np.multiply(e,aa))\n",
    "    dxy = np.add(np.multiply(fx,dfY),np.multiply(e,bb))\n",
    "    dyy = np.add(np.multiply(fy,dfY),2.0*np.multiply(e,cc))\n",
    "    \n",
    "    #,dxx.sum(),dxy.sum(),dyy.sum())\n",
    "\n",
    "    return e.sum(),fx.sum(),fy.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=51\n",
    "x=np.linspace(-2.5, 2.5,size)\n",
    "y=np.linspace(-2.5, 2.5,size)\n",
    "xv, yv = np.meshgrid(x, y, sparse=False, indexing='xy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fa6e6479be0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "e=energy_mesh_2(xv,yv)\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "normalize1 = colors.Normalize(vmin=np.min(e), vmax=6)\n",
    "#normalize2 = colors.Normalize(vmin=-50, vmax=50)\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "ax.scatter(xv,yv,c=e,cmap = colormap,norm=normalize1,marker='s',s=20)\n",
    "#ax.scatter(xv,yv,c=fx,cmap = colormap,norm=normalize2,alpha=0.1)\n",
    "#ax.scatter(xv,yv,c=fy,cmap = colormap,norm=normalize2,alpha=0.1)\n",
    "#ax.set_ylim(-1,2.5)\n",
    "#ax.set_xlim(-2,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0 -1.0 -1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(e))\n",
    "result = numpy.where(e == numpy.amin(e))\n",
    "r0 = list(zip(result[0],result[1]))\n",
    "x0 = xv[r0[0][0]][r0[0][1]]\n",
    "y0 = yv[r0[0][0]][r0[0][1]]\n",
    "print(energy_mesh_2(x0,y0),x0,y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "left=[1,0,0,0]\n",
    "right=[0,1,0,0]\n",
    "up =[0,0,1,0]\n",
    "down = [0,0,0,1]\n",
    "possible_actions = ['left','right','up','down']\n",
    "\n",
    "state_size = 2\n",
    "action_size = 4\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Example Penalites/Rewards to Consider\n",
    "# 1. Going back to the same points -1\n",
    "# 2. Going back to a minimum: -100\n",
    "# 3. Each successive step: -0.5 \n",
    "# 4a. Going uphill {delta_E > 0}: exp(-delta_E*alpha) or -alpha*delta_E + 1\n",
    "# 4b. Go downhiil {delta_E < 0}: exp(delta_E*alpha) or alpha*delta_E + 1\n",
    "# 4c. Plateau {delta_E = 0}: 0 (discourage wandering on a plateau)\n",
    "# This ensures there is equal reward for small climbs and small descents\n",
    "# Larger changes in energy are discouraged\n",
    "# 5. If 200 steps have reached, terminate\n",
    "# 6. If get to other minimum: +100\n",
    "# 7. If get to saddle point: +50 (ensures climbing)\n",
    "# The most difficult balance is obtaining a short path but ensuring the path is the minimum energy\n",
    "# The successive step penalty ensures the path is short\n",
    "# The energy penalty ensures small energy changes are rewarded. \n",
    "\n",
    "\n",
    "class Board:\n",
    "    def __init__(self, step):\n",
    "        self.step = step \n",
    "        self.x0 = 0.0\n",
    "        self.y0 = 0.0\n",
    "        self.energy = energy_point_2(self.x0,self.y0)\n",
    "        self.reward = 0.0\n",
    "        self.alpha = 2.0\n",
    "        self.xmin = -1.0\n",
    "        self.ymin = -1.0\n",
    "        self.done = False\n",
    "        self.xend = 1.0\n",
    "        self.yend = 1.0\n",
    "        self.xsad = 0.0\n",
    "        self.ysad = 0.0\n",
    "        \n",
    "    def get_reward(self, action):\n",
    "        xtemp = self.x0\n",
    "        ytemp = self.y0\n",
    "        etemp = self.energy\n",
    "        print(etemp)\n",
    "        if(action == 'left'):\n",
    "            self.x0 -= self.step\n",
    "        if(action == 'right'):\n",
    "            self.x0 += self.step\n",
    "        if(action == 'up'):\n",
    "            self.y0 += self.step\n",
    "        if(action == 'down'):\n",
    "            self.y0 -= self.step\n",
    "        \n",
    "        self.energy = energy_point_2(self.x0,self.y0)\n",
    "        delta_E = self.energy - etemp\n",
    "        print(delta_E)\n",
    "        if(delta_E == 0):\n",
    "            self.reward = 0.0\n",
    "        if(delta_E < 0):\n",
    "            self.reward = math.exp(delta_E*self.alpha)\n",
    "        if(delta_E > 0): \n",
    "            self.reward = math.exp(-delta_E*self.alpha)\n",
    "        \n",
    "        if((self.x0,self.y0)==(xtemp,ytemp)):\n",
    "            self.reward -= 0.01\n",
    "        if((self.x0,self.y0)==(self.xmin,self.ymin)):\n",
    "            self.reward -= 1\n",
    "            self.done = True\n",
    "        if((self.x0,self.y0)==(self.xend,self.yend)):\n",
    "            self.reward += 1\n",
    "            self.done = True\n",
    "        \n",
    "        self.reward -= 0.05\n",
    "        return self.reward\n",
    "            \n",
    "    def get_done(self):\n",
    "        return self.done\n",
    "    \n",
    "    def end_sess(self):\n",
    "        self.done = True\n",
    "    \n",
    "    def action(self,action):\n",
    "        reward = self.get_reward(action)\n",
    "        done = self.get_done()\n",
    "        return reward, done, self.x0, self.y0\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQModel:\n",
    "    def __init__(self, state_size,action_size,learning_rate,name=\"DQModel\"):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32,[None, *state_size],name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32,[None, 4],name=\"actions_\")\n",
    "            \n",
    "            self.target_Q = tf.placeholder(tf.float32,[None],name='target')\n",
    "            \n",
    "            self.d1 = tf.layers.Dense(inputs=self.inputs_,\n",
    "                                      units = 20,\n",
    "                                      activation='sigmoid',\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=\"d1\")\n",
    "            \n",
    "            self.d2 = tf.layers.Dense(inputs=self.d1,\n",
    "                                      units = 20,\n",
    "                                      activation='sigmoid',\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=\"d2\")\n",
    "            \n",
    "            self.output = tf.layers.Dense(inputs = self.d2, \n",
    "                                          units=4,\n",
    "                                          activation='linear',\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"output\")\n",
    "            \n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.6875\n",
      "(-0.24716040419525354, False, 0.5, 0.0)\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "#DQModel = DQModel(state_size, action_size, learning_rate)\n",
    "b = Board(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps = 1000\n",
    "dx = 0.2\n",
    "\n",
    "e_old = energy(x0,y0)\n",
    "beta = 1.0\n",
    "\n",
    "xO = x0\n",
    "yO = y0\n",
    "acc_rate = 0\n",
    "traj = np.empty((nsteps,2))\n",
    "print(traj)\n",
    "for i in range (nsteps):\n",
    "    xN = xO + (random.uniform(0,1)-0.5)*dx/math.sqrt(2)\n",
    "    yN = yO + (random.uniform(0,1)-0.5)*dx/math.sqrt(2)\n",
    "    \n",
    "    e = energy(xN,yN)\n",
    "    dE = e - e_old \n",
    "    acc = False\n",
    "    \n",
    "    if(dE <= 0):\n",
    "        acc = True\n",
    "    else:\n",
    "        P = exp(-beta*dE)\n",
    "        if P >= random.uniform(0,1):\n",
    "            acc = True\n",
    "        else:\n",
    "            acc = False\n",
    "    \n",
    "    if acc:\n",
    "        xO = xN\n",
    "        yO = yN\n",
    "        e_old = e\n",
    "        acc_rate += 1\n",
    "    \n",
    "    traj[i][0] = xO\n",
    "    traj[i][1] = yO\n",
    "        \n",
    "print(traj) \n",
    "e=energy(xv,yv)\n",
    "plt.scatter(xv,yv,c=e,cmap = colormap,norm=normalize)\n",
    "plt.scatter(traj[:,0],traj[:,1],s=2,c='white')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
